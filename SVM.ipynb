{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "import random\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/A_DEFORMED1_C0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yd/mh_zcn295tl15dz8w8jr2p7h0000gn/T/ipykernel_12888/2720044415.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrx01\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'A_RX1_C0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdeformed01\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_rx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrx01\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/A_DEFORMED1_C0.csv'"
     ]
    }
   ],
   "source": [
    "data_folder = './data/'\n",
    "\n",
    "#CSV filenames\n",
    "deformed01 = 'A_DEFORMED1_C0'\n",
    "rx01 = 'A_RX1_C0'\n",
    "\n",
    "df_def = pd.read_csv(data_folder + deformed01 + '.csv', header = None)\n",
    "df_rx = pd.read_csv(data_folder + rx01 + '.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the frequency of the signal is 3.04878048780488 MHz\n",
      "the total duration of the signal is 10.495999999999993 seconds\n",
      "number of samples : 32000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#initialisation of clock variables\n",
    "clock = 3048780.48780488\n",
    "delay_in_samples = 304878\n",
    "g_T = 1 / clock\n",
    "g_N = len(df_def)\n",
    "\n",
    "print(\"the frequency of the signal is\", clock * 1e-6, \"MHz\")\n",
    "print(\"the total duration of the signal is\", g_N / clock, \"seconds\")\n",
    "print(\"number of samples :\", len(df_def))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"Removes the silence at the end of the audio\n",
    "\n",
    "    Args:\n",
    "        df (array[][]): the data from the csv\n",
    "\n",
    "    Returns:\n",
    "        array[][]: the data without the silence at the end\n",
    "    \"\"\"\n",
    "    epsilon =  1e-1\n",
    "    #remove the silence at the end: \n",
    "    for i in range(len(df) - 1, 0, -1):\n",
    "        if(df[0][i] > epsilon):\n",
    "            print(\"deleted\", (len(df) - i) / clock, \"samples from the end because we assume it is silence from\", len(df) / clock, \"samples\")\n",
    "            return df[:i]\n",
    "\n",
    "def split_csv(filename):\n",
    "    \"\"\"Function that divides the data set in samples of 126ms \n",
    "\n",
    "    Args:\n",
    "        filename (csv): The csv containing the data\n",
    "\n",
    "    Returns:\n",
    "        array[][]: the signal split in samples of 126ms\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_folder + filename + '.csv', header = None)\n",
    "    df = clean_data(df)\n",
    "    df.rename(columns = {0:'amplitude'}, inplace = True)\n",
    "    point_per_sample = int(160e-3 * clock)\n",
    "    \n",
    "    nb_of_samples = int(len(df)/point_per_sample)\n",
    "    data = df[:nb_of_samples * point_per_sample]\n",
    "    return np.array_split(data, nb_of_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def wavelet_transform(data,widths):  \n",
    "    \"\"\"Computes the Continuous Wavelet Transform of the signal\n",
    "\n",
    "    Args:\n",
    "        data (2 dimensional array): The signal on which we want to perform the Continuous Wavelet Transform\n",
    "        widths ([type]): [description]\n",
    "\n",
    "    Returns:\n",
    "        (M,) sequence: The width used for the transform\n",
    "    \"\"\"\n",
    "    cwt = []\n",
    "    for i in data:\n",
    "        cwt.append(np.transpose(signal.cwt(i['amplitude'], signal.ricker, widths)))\n",
    "\n",
    "    return cwt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(cwt_def,cwt_rx):\n",
    "    \"\"\" Aggregates the slices of 126ms into one signal\n",
    "\n",
    "    Args:\n",
    "        cwt_def (3-dim array): array we want to flatten to 2-dim\n",
    "        cwt_rx (3-dim array): array we want to flatten to 2-dim\n",
    "    Returns:\n",
    "        (array): 2-dim array\n",
    "        (array): 2-dim array\n",
    "    \"\"\"\n",
    "    deformed = []\n",
    "    for sublist in cwt_def:\n",
    "        for item in sublist:\n",
    "            deformed.append(item)\n",
    "\n",
    "    rx = []\n",
    "    for sublist in cwt_rx:\n",
    "        for item in sublist:\n",
    "            rx.append(item)\n",
    "    return deformed,rx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_test_train(deformed_data,rx_data):\n",
    "    \"\"\"Generates the train and test sets by selecting random portions of the signal with no overlap\n",
    "        at a 70-30% ratio\n",
    "\n",
    "    Args:\n",
    "        train_len (int): length of the train set\n",
    "        test_len (int): length of the test set\n",
    "\n",
    "    Returns:\n",
    "       (array,array): train set and test set and corresponding targets\n",
    "    \"\"\"\n",
    "    # Threshold at which the svm runs in an acceptable time\n",
    "    max_tresh = 70000\n",
    "    train_len= max_tresh//2\n",
    "    test_len = int(((train_len*.3)/.7)//2)\n",
    "    rand_train = random.randrange(0,len(rx))\n",
    "    rand_test =  random.randrange(0,len(rx))\n",
    "    # The test and train set do not overlap\n",
    "    while (rand_test in range(rand_train, rand_train + train_len)):\n",
    "         rand_test =  random.randrange(0,len(rx), train_len)\n",
    "    return  np.concatenate((deformed_data[rand_train:rand_train + train_len],rx_data[rand_train:rand_train + train_len])) ,np.concatenate((deformed_data[rand_test:rand_test+test_len],rx_data[rand_test:rand_test+test_len])), np.concatenate((np.ones(train_len),np.zeros(train_len))),np.concatenate((np.ones(test_len),np.zeros(test_len)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize the data before feeding it to the PCA algorithm\n",
    "def standardize(train_set, test_set):\n",
    "    \"\"\"Standarize the data before feeding it to the PCA algorithm\n",
    "\n",
    "    Args:\n",
    "        train_set (array like (n-samples, n_features): The train set\n",
    "        test_set (array like (n-samples, n_features)): The test set\n",
    "\n",
    "    Returns:\n",
    "       (array (n-samples, n_features)): the standardized train set  \n",
    "       (array (n-samples, n_features)): the standardized test set\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    # Fit on training set only\n",
    "    scaler.fit(train_set)\n",
    "    # Apply transform to both the training set and the test set\n",
    "    return scaler.transform(train_set), scaler.transform(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(train,test):\n",
    "    \"\"\"PCA for feature extraction on the standardized data\n",
    "\n",
    "    Args:\n",
    "        train (array (n-samples, n_features)): train set we want to extract features from\n",
    "        test (array (n-samples, n_features)): test set we want to extract features from\n",
    "\n",
    "    Returns:\n",
    "        (array (n-samples, n_features)): dimension-reduced train set \n",
    "        (array (n-samples, n_features)): dimension-reduced test-set \n",
    "    \"\"\"\n",
    "    #We chose the minimal number of prinicpal component such that 95% of the variance is retained\n",
    "    pca = PCA(.95)\n",
    "    #fit the data onto the vectors computed by the algorithm\n",
    "    pca.fit(train)\n",
    "    print(pca.n_components_ ,\"components were used to capture the data instead of\", train.shape[1])\n",
    "    #does the projection\n",
    "    return pca.transform(train),pca.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runSVM(train,test,target,kernel):\n",
    "    \"\"\"SVM \n",
    "\n",
    "    Args:\n",
    "        train ((array (n-samples, n_features)): train set\n",
    "        test ((array (n-samples, n_features)): test set\n",
    "        target ((array (n-samples,)): target\n",
    "        kernel (string): kernel to use for the SVM\n",
    "\n",
    "    Returns:\n",
    "        (array (n-samples,): prediction given by the SVM\n",
    "    \"\"\"\n",
    "    #Create a svm Classifier\n",
    "    clf = svm.SVC(kernel=kernel)\n",
    "    #Train the model using the training sets\n",
    "    clf.fit(train, target)\n",
    "    #Predict the response for test dataset\n",
    "    return clf.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getStatistics(y_test, y_pred):\n",
    "    \"\"\"Function to compute accuracy,recall,f1-score and more\n",
    "\n",
    "    Args:\n",
    "        y_test ((array (n-samples,)): test set target\n",
    "        y_pred ((array (n-samples,)): classification given by the model\n",
    "    \"\"\"\n",
    "    # Model Accuracy: how often is the classifier correct?\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "    # Model Recall: what percentage of positive tuples are labelled as such?\n",
    "    print(\"Recall:\", metrics.recall_score(y_test, y_pred))\n",
    "    print(\"Confusion matrix\", confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted 1.3556525359999991 samples from the end because we assume it is silence from 10.495999999999993 samples\n",
      "deleted 1.2826049679999991 samples from the end because we assume it is silence from 10.495999999999993 samples\n",
      "4 components were used to capture the data instead of 29\n",
      "Accuracy: 0.5010666666666667\n",
      "Precision: 0.5005381407237993\n",
      "Recall: 0.9921333333333333\n",
      "Confusion matrix [[  75 7425]\n",
      " [  59 7441]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.01      0.02      7500\n",
      "         1.0       0.50      0.99      0.67      7500\n",
      "\n",
      "    accuracy                           0.50     15000\n",
      "   macro avg       0.53      0.50      0.34     15000\n",
      "weighted avg       0.53      0.50      0.34     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the data split in 126 ms slices\n",
    "data_deformed = split_csv(deformed01)\n",
    "data_rx = split_csv(rx01)\n",
    "#Compute the wavelet transform of each slice\n",
    "cwt_def = wavelet_transform(data_deformed,np.arange(1,30))\n",
    "cwt_rx = wavelet_transform(data_rx,np.arange(1,30))\n",
    "#Agregate the slices into one signal\n",
    "deformed, rx = flatten_data(cwt_def,cwt_rx)\n",
    "#Compute train and test set\n",
    "X_train, X_test,y_train,y_test = generate_test_train(deformed,rx)\n",
    "#Standardize the data\n",
    "stand_train,stand_test = standardize(X_train,X_test)\n",
    "# Perform dimensionality reduction on the data\n",
    "pca_train,pca_test= feature_extraction(stand_test,stand_train)\n",
    "#Run the SVM\n",
    "prediction = runSVM(stand_train,stand_test,y_train,'rbf')\n",
    "getStatistics(y_test,prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the SVM with the neural net features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Librosa_features(filename):\n",
    "    \"\"\"Gets the data and features generated by Librosa, a python package for music and audio analysis and standardizes it\n",
    "       The features have been generated in the SequentialNN.ipynb file\n",
    "    Args:\n",
    "        filename (string): file name where the data is contained\n",
    "\n",
    "    Returns:\n",
    "        array: standardized data\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(filename)\n",
    "    data = data.drop(['filename'],axis=1)\n",
    "    labels = data.iloc[:, -1]\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float)),encoder.fit_transform(labels)\n",
    "\n",
    "\n",
    "def train_test_set(data):\n",
    "    \"\"\"Splits the data into 70-30% train-test sets\n",
    "\n",
    "    Args:\n",
    "        data (array (n-samples, n_features)): data with the features from\n",
    "\n",
    "    Returns:\n",
    "        [array]: the train and test set and thei corresponding targets\n",
    "    \"\"\"\n",
    "    train_len = int((len(data)//2)*0.7)\n",
    "    test_len = (len(data) //2 )- train_len\n",
    "    mid = len(data)//2\n",
    "   \n",
    "\n",
    "    X_train = np.concatenate((data[:train_len],data[mid:mid+train_len]))\n",
    "    X_test = np.concatenate((data[train_len:mid],data[mid+train_len:]))\n",
    "    y_train=np.concatenate((np.ones(train_len),np.zeros(train_len)))\n",
    "    y_test=np.concatenate((np.ones(test_len),np.zeros(test_len)))\n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "\n",
    "X,y = get_Librosa_features(\"data.csv\")\n",
    "x_train,x_test,y_train,y_test= train_test_set(X)\n",
    "pred_nn = runSVM(x_train,x_test,y_train,\"rbf\")\n",
    "getStatistics(pred_nn,y_test)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1dd2691ec88a8b563fe6b792cd5b35b2471a11a89b348b9101d9c34f3b232cce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
